{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20a1309-8f71-4008-8470-fabbfa681c13",
   "metadata": {},
   "source": [
    "# 002 - Simple Neural Network\n",
    "\n",
    "\n",
    "Following the example in https://pytorch.org/tutorials/advanced/cpp_frontend.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cc3f45-667f-4019-8b24-9f539212349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel initialized\n",
      "torch initialized"
     ]
    }
   ],
   "source": [
    "#include<iostream>\n",
    "#include<string>\n",
    "std::cerr << \"Kernel initialized\\n\";\n",
    "\n",
    "// Setup: TODO simplify this\n",
    "// add headers\n",
    "#pragma cling add_include_path(\"../libs/libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_include_path(\"../libs/libtorch/include/\")\n",
    "// add precompiled libtorch*.so files\n",
    "#pragma cling add_library_path(\"../libs/libtorch/lib/\")\n",
    "#pragma cling load(\"libtorch.so\")\n",
    "\n",
    "//// --- setup is ready -- time to import torch\n",
    "#include <torch/torch.h>\n",
    "std::cerr << \"torch initialized\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09653613-5367-48c2-be36-a44a598b71a5",
   "metadata": {},
   "source": [
    "## Simple network\n",
    "\n",
    "\n",
    "Reference: https://pytorch.org/cppdocs/frontend.html#end-to-end-example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d79fd16-0b1b-4bef-b06b-b0c07cb582e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/tg/work/repos/me/libtorch-tutorial/data/MNIST/raw/._OK exists. skipping\n"
     ]
    }
   ],
   "source": [
    "!bash ../data/setup.sh  to download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c0f01a-7c46-428b-a2f1-cee5b39c64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch: 100 | Loss: 2.29083\n",
      "Epoch: 1 | Batch: 200 | Loss: 2.17135\n",
      "Epoch: 1 | Batch: 300 | Loss: 2.21064\n",
      "Epoch: 1 | Batch: 400 | Loss: 2.06644\n",
      "Epoch: 1 | Batch: 500 | Loss: 1.89153\n",
      "Epoch: 1 | Batch: 600 | Loss: 1.92552\n",
      "Epoch: 1 | Batch: 700 | Loss: 1.74535\n",
      "Epoch: 1 | Batch: 800 | Loss: 1.40254\n",
      "Epoch: 1 | Batch: 900 | Loss: 1.31521\n",
      "Epoch: 2 | Batch: 100 | Loss: 1.0744\n",
      "Epoch: 2 | Batch: 200 | Loss: 1.08279\n",
      "Epoch: 2 | Batch: 300 | Loss: 1.16592\n",
      "Epoch: 2 | Batch: 400 | Loss: 0.836995\n",
      "Epoch: 2 | Batch: 500 | Loss: 0.938627\n",
      "Epoch: 2 | Batch: 600 | Loss: 1.11453\n",
      "Epoch: 2 | Batch: 700 | Loss: 0.696703\n",
      "Epoch: 2 | Batch: 800 | Loss: 0.69518\n",
      "Epoch: 2 | Batch: 900 | Loss: 0.640828\n",
      "Epoch: 3 | Batch: 100 | Loss: 0.905752\n",
      "Epoch: 3 | Batch: 200 | Loss: 0.755499\n",
      "Epoch: 3 | Batch: 300 | Loss: 0.809715\n",
      "Epoch: 3 | Batch: 400 | Loss: 0.730176\n",
      "Epoch: 3 | Batch: 500 | Loss: 0.756368\n",
      "Epoch: 3 | Batch: 600 | Loss: 0.70157\n",
      "Epoch: 3 | Batch: 700 | Loss: 0.739328\n",
      "Epoch: 3 | Batch: 800 | Loss: 0.720477\n",
      "Epoch: 3 | Batch: 900 | Loss: 0.535844\n",
      "Epoch: 4 | Batch: 100 | Loss: 0.610563\n",
      "Epoch: 4 | Batch: 200 | Loss: 0.758208\n",
      "Epoch: 4 | Batch: 300 | Loss: 0.402054\n",
      "Epoch: 4 | Batch: 400 | Loss: 0.561277\n",
      "Epoch: 4 | Batch: 500 | Loss: 0.554144\n",
      "Epoch: 4 | Batch: 600 | Loss: 0.484097\n",
      "Epoch: 4 | Batch: 700 | Loss: 0.503595\n",
      "Epoch: 4 | Batch: 800 | Loss: 0.579294\n",
      "Epoch: 4 | Batch: 900 | Loss: 0.788531\n",
      "Epoch: 5 | Batch: 100 | Loss: 0.597506\n",
      "Epoch: 5 | Batch: 200 | Loss: 0.509249\n",
      "Epoch: 5 | Batch: 300 | Loss: 0.532219\n",
      "Epoch: 5 | Batch: 400 | Loss: 0.58322\n",
      "Epoch: 5 | Batch: 500 | Loss: 0.63367\n",
      "Epoch: 5 | Batch: 600 | Loss: 0.497622\n",
      "Epoch: 5 | Batch: 700 | Loss: 0.452713\n",
      "Epoch: 5 | Batch: 800 | Loss: 0.495629\n",
      "Epoch: 5 | Batch: 900 | Loss: 0.32615\n",
      "Epoch: 6 | Batch: 100 | Loss: 0.224445\n",
      "Epoch: 6 | Batch: 200 | Loss: 0.49674\n",
      "Epoch: 6 | Batch: 300 | Loss: 0.424574\n",
      "Epoch: 6 | Batch: 400 | Loss: 0.490835\n",
      "Epoch: 6 | Batch: 500 | Loss: 0.340995\n",
      "Epoch: 6 | Batch: 600 | Loss: 0.393731\n",
      "Epoch: 6 | Batch: 700 | Loss: 0.296038\n",
      "Epoch: 6 | Batch: 800 | Loss: 0.276948\n",
      "Epoch: 6 | Batch: 900 | Loss: 0.353581\n",
      "Epoch: 7 | Batch: 100 | Loss: 0.477266\n",
      "Epoch: 7 | Batch: 200 | Loss: 0.745573\n",
      "Epoch: 7 | Batch: 300 | Loss: 0.660802\n",
      "Epoch: 7 | Batch: 400 | Loss: 0.46544\n",
      "Epoch: 7 | Batch: 500 | Loss: 0.58793\n",
      "Epoch: 7 | Batch: 600 | Loss: 0.427319\n",
      "Epoch: 7 | Batch: 700 | Loss: 0.431962\n",
      "Epoch: 7 | Batch: 800 | Loss: 0.429974\n",
      "Epoch: 7 | Batch: 900 | Loss: 0.387299\n",
      "Epoch: 8 | Batch: 100 | Loss: 0.294752\n",
      "Epoch: 8 | Batch: 200 | Loss: 0.512965\n",
      "Epoch: 8 | Batch: 300 | Loss: 0.565091\n",
      "Epoch: 8 | Batch: 400 | Loss: 0.445624\n",
      "Epoch: 8 | Batch: 500 | Loss: 0.457231\n",
      "Epoch: 8 | Batch: 600 | Loss: 0.50093\n",
      "Epoch: 8 | Batch: 700 | Loss: 0.337358\n",
      "Epoch: 8 | Batch: 800 | Loss: 0.305645\n",
      "Epoch: 8 | Batch: 900 | Loss: 0.576167\n",
      "Epoch: 9 | Batch: 100 | Loss: 0.418367\n",
      "Epoch: 9 | Batch: 200 | Loss: 0.461544\n",
      "Epoch: 9 | Batch: 300 | Loss: 0.317391\n",
      "Epoch: 9 | Batch: 400 | Loss: 0.268952\n",
      "Epoch: 9 | Batch: 500 | Loss: 0.626361\n",
      "Epoch: 9 | Batch: 600 | Loss: 0.291088\n",
      "Epoch: 9 | Batch: 700 | Loss: 0.28522\n",
      "Epoch: 9 | Batch: 800 | Loss: 0.373888\n",
      "Epoch: 9 | Batch: 900 | Loss: 0.182346\n",
      "Epoch: 10 | Batch: 100 | Loss: 0.552429\n",
      "Epoch: 10 | Batch: 200 | Loss: 0.381942\n",
      "Epoch: 10 | Batch: 300 | Loss: 0.311408\n",
      "Epoch: 10 | Batch: 400 | Loss: 0.397417\n",
      "Epoch: 10 | Batch: 500 | Loss: 0.331404\n",
      "Epoch: 10 | Batch: 600 | Loss: 0.615582\n",
      "Epoch: 10 | Batch: 700 | Loss: 0.340934\n",
      "Epoch: 10 | Batch: 800 | Loss: 0.448931\n",
      "Epoch: 10 | Batch: 900 | Loss: 0.516899\n"
     ]
    }
   ],
   "source": [
    "auto mnist_data_dir = \"../data/MNIST/raw\";\n",
    "\n",
    "// Define a new Module.\n",
    "struct Net : torch::nn::Module {\n",
    "  Net() {\n",
    "    // Construct and register two Linear submodules.\n",
    "    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));\n",
    "    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n",
    "    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n",
    "  }\n",
    "\n",
    "  // Implement the Net's algorithm.\n",
    "  torch::Tensor forward(torch::Tensor x) {\n",
    "    // Use one of many tensor manipulation functions.\n",
    "    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));\n",
    "    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());\n",
    "    x = torch::relu(fc2->forward(x));\n",
    "    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);\n",
    "    return x;\n",
    "  }\n",
    "\n",
    "  // Use one of many \"standard library\" modules.\n",
    "  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n",
    "};\n",
    "\n",
    "\n",
    "// Create a new Net.\n",
    "auto net = std::make_shared<Net>();\n",
    "\n",
    "// Create a multi-threaded data loader for the MNIST dataset.\n",
    "auto data_loader = torch::data::make_data_loader(\n",
    "  torch::data::datasets::MNIST(mnist_data_dir).map(\n",
    "      torch::data::transforms::Stack<>()),\n",
    "  /*batch_size=*/64);\n",
    "\n",
    "// Instantiate an SGD optimization algorithm to update our Net's parameters.\n",
    "torch::optim::SGD optimizer(net->parameters(), /*lr=*/0.01);\n",
    "\n",
    "for (size_t epoch = 1; epoch <= 10; ++epoch) {\n",
    "    size_t batch_index = 0;\n",
    "    // Iterate the data loader to yield batches from the dataset.\n",
    "    for (auto& batch : *data_loader) {\n",
    "      // Reset gradients.\n",
    "      optimizer.zero_grad();\n",
    "      // Execute the model on the input data.\n",
    "      torch::Tensor prediction = net->forward(batch.data);\n",
    "      // Compute a loss value to judge the prediction of our model.\n",
    "      torch::Tensor loss = torch::nll_loss(prediction, batch.target);\n",
    "      // Compute gradients of the loss w.r.t. the parameters of our model.\n",
    "      loss.backward();\n",
    "      // Update the parameters based on the calculated gradients.\n",
    "      optimizer.step();\n",
    "      // Output the loss and checkpoint every 100 batches.\n",
    "      if (++batch_index % 100 == 0) {\n",
    "        std::cout << \"Epoch: \" << epoch << \" | Batch: \" << batch_index\n",
    "                  << \" | Loss: \" << loss.item<float>() << std::endl;\n",
    "        // Serialize your model periodically as a checkpoint.\n",
    "        torch::save(net, \"net.pt\");\n",
    "      }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb53178-4c3e-4beb-aeb6-ea0f63c09a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
